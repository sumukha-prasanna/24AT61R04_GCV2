# -*- coding: utf-8 -*-
"""MonteCarlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bk5XcSaFPaB0tU49ZFIQlgtEQsNQp0XQ
"""

"""
24AT61R04
Sumukha Prasanna Kumar
GCV2 - Grass Cutting on a Sunny Day [Version 2]
"""
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from LawnEnv import LawnEnv

def mc_prediction(env, num_episodes, discount_factor=1.0):
    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)
    V = defaultdict(float)
    total_reward = 0

    for i_episode in range(1, num_episodes + 1):
        if i_episode % 1000 == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")

        episode = []
        state = env.reset()
        episode_reward = 0
        for t in range(100):
            action = select_action_proximity_policy(env)
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            episode_reward += reward
            if done:
                break
            state = next_state

        total_reward += episode_reward

        states_in_episode = set([tuple(x[0]) for x in episode])
        for state in states_in_episode:
            first_occurence_idx = next(i for i, x in enumerate(episode) if x[0] == state)
            G = sum([x[2] * (discount_factor ** i) for i, x in enumerate(episode[first_occurence_idx:])])
            returns_sum[state] += G
            returns_count[state] += 1.0
            V[state] = returns_sum[state] / returns_count[state]

    return V, total_reward

def select_action_proximity_policy(env):
    farmer_pos = env.farmer_pos

    adjacent_cells = [
        (farmer_pos[0] - 1, farmer_pos[1]),
        (farmer_pos[0] + 1, farmer_pos[1]),
        (farmer_pos[0], farmer_pos[1] - 1),
        (farmer_pos[0], farmer_pos[1] + 1)
    ]

    valid_uncut_grass = []
    for cell in adjacent_cells:
        row, col = cell
        if 0 <= row < env.N and 0 <= col < env.M and env.grid[row, col] == 0:
            valid_uncut_grass.append(cell)

    if valid_uncut_grass:
        next_position = 0
        if len(valid_uncut_grass) >= 2:
            next_position = np.random.randint(0, len(valid_uncut_grass) - 1)
        nearest_grass = valid_uncut_grass[next_position]
        if nearest_grass[0] < farmer_pos[0]:
            return 0
        elif nearest_grass[0] > farmer_pos[0]:
            return 2
        elif nearest_grass[1] < farmer_pos[1]:
            return 3
        else:
            return 1

    return np.random.choice([0, 1, 2, 3])

def visualize_lawn(env, episode, bottles_collected):
    plt.figure(figsize=(8, 8))
    plt.imshow(env.grid, cmap='Greens', interpolation='nearest')
    plt.colorbar(label='Grass Status (0=uncut, 1=cut)')
    plt.scatter(env.farmer_pos[1], env.farmer_pos[0], color='red', s=200, label='Farmer')

    for bottle in env._place_water_bottles():
        plt.scatter(bottle[1], bottle[0], color='blue', s=200, label='Water Bottle')

    plt.title(f'Episode: {episode}, Bottles Collected: {bottles_collected}')
    plt.xlabel('Columns')
    plt.ylabel('Rows')
    plt.xticks(np.arange(env.M))
    plt.yticks(np.arange(env.N))
    plt.grid()
    plt.legend()
    plt.show()

if __name__ == "__main__":
    env = LawnEnv(N=5, M=5, K=3, T=20, t=5)

    episodes = 10000

    print("Running Monte Carlo Prediction...")
    V, total_reward = mc_prediction(env, num_episodes=episodes)

    print("\nTotal reward after ", episodes ," episodes:", total_reward)

    print("Visualizing environment before policy iteration...")
    visualize_lawn(env, episode=10000, bottles_collected=3)
    if env.done:
        print("Farmer completed work")
    else:
        print("Farmer fainted")